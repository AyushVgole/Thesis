{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ddXrmcoFRsE"
      },
      "outputs": [],
      "source": [
        "# Import all library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset1_path = '/content/drive/MyDrive/Untitled folder/Custom dataset/Face/Preporcessed'\n",
        "dataset2_path = '/content/drive/MyDrive/Untitled folder/CK+48/CK+48'\n",
        "# Define face image size\n",
        "img_width, img_height = 48,48\n",
        "\n",
        "# batch size\n",
        "batch_size = 32\n",
        "# Create ImageDataGenerator instances for both datasets\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "train_generator1 = datagen.flow_from_directory(\n",
        "    dataset1_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_generator2 = datagen.flow_from_directory(\n",
        "    dataset2_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlCq6R3eNB35",
        "outputId": "8e4f9bb2-af8a-4e5e-c67f-8f778d791772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 images belonging to 7 classes.\n",
            "Found 981 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKohN_SWISL4",
        "outputId": "4148d695-0308-40b8-8e8d-c0e3e2b88985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create Combined generator\n",
        "def combined_generator(gen1, gen2):\n",
        "    while True:\n",
        "        try:\n",
        "            x1, y1 = next(gen1)  # Try to get data from generator1\n",
        "        except StopIteration:\n",
        "            x1, y1 = None, None  # If generator1 is exhausted, set to None\n",
        "        try:\n",
        "            x2, y2 = next(gen2)  # Try to get data from generator2\n",
        "        except StopIteration:\n",
        "            break  # If generator2 is exhausted, break the loop\n",
        "\n",
        "        if x1 is not None and x2 is not None:  # If both generators have data\n",
        "            yield np.concatenate([x1, x2]), np.concatenate([y1, y2])\n",
        "\n",
        "combined_generator = combined_generator(train_generator1, train_generator2)\n",
        "\n"
      ],
      "metadata": {
        "id": "P1oW4lQFNE6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "fQ4yw9O1NG9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "for layer in base_model.layers[-10:]:    # Freezing the base model layers\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(7, activation='softmax')(x)  #number of layers as per classes for emotions\n",
        "\n",
        "\n",
        "model1 = Model(inputs=base_model.input, outputs=x)  # Creating the final model\n",
        "\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "for _ in range(len(train_generator1) + len(train_generator2)):\n",
        "    next(combined_generator)  # Consume data until exhaustion\n",
        "# fitting the model\n",
        "model1.fit(combined_generator,\n",
        "          steps_per_epoch=max(len(train_generator1), len(train_generator2)),\n",
        "          epochs=5,\n",
        "          validation_data=combined_generator,\n",
        "          validation_steps=max(len(train_generator1), len(train_generator2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjl7GfU_NKQm",
        "outputId": "58bf4967-f3ed-43f2-bfe1-297d961560f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 8s/step - accuracy: 0.4119 - loss: 1.8040 - val_accuracy: 0.1028 - val_loss: 116.0117\n",
            "Epoch 2/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 8s/step - accuracy: 0.8129 - loss: 0.6135 - val_accuracy: 0.1004 - val_loss: 319.1422\n",
            "Epoch 3/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 8s/step - accuracy: 0.8746 - loss: 0.4169 - val_accuracy: 0.1093 - val_loss: 3.8572\n",
            "Epoch 4/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 8s/step - accuracy: 0.9232 - loss: 0.2743 - val_accuracy: 0.1735 - val_loss: 2.7638\n",
            "Epoch 5/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 8s/step - accuracy: 0.9297 - loss: 0.2791 - val_accuracy: 0.1037 - val_loss: 4.1696\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4ea1fa7d00>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the base model's input and output layers\n",
        "feature_extraction_model = Model(inputs=base_model.input, outputs=base_model.output)"
      ],
      "metadata": {
        "id": "IYXJquY_OYXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming data is preprocessed and batched\n",
        "features = feature_extraction_model.predict(train_generator2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X9Xms_s-4m1",
        "outputId": "b5d2ee73-8173-4106-cc02-37974bd8bda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 427ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "data_dir = '/content/drive/MyDrive/Untitled folder/Custom dataset/Game scne/Test4'    # Define data directory path\n",
        "\n",
        "# Create ImageDataGenerator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(150, 100),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSeAlN8p-6ab",
        "outputId": "bd00a756-912b-4059-d5ea-c6e06025c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 193 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print(data_batch.shape)  # verify batch size\n",
        "    print(labels_batch.shape)  # verify batch size\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pugHvt36_E8o",
        "outputId": "8619c8e2-4caa-443e-c3cd-6a168e26742e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 150, 100, 3)\n",
            "(8, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained ResNet model without the top layers\n",
        "base_model2 = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 100, 3))\n",
        "# Freeze the base model layers\n",
        "for layer in base_model2.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "ebOzPCYeAPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom top layers\n",
        "x = base_model2.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(126, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(7, activation='softmax')(x)  #number of layers as per classes for emotions\n",
        "\n",
        "# Create the final model\n",
        "model2 = Model(inputs=base_model2.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "CLehOrEmAfBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] )\n",
        "# Fitting the model\n",
        "history = model2.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs= 5  # Adjust the number of epochs as need\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf63aIfmAjwz",
        "outputId": "c0ea6bca-3a12-47ce-e5b7-fee4f306a915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 637ms/step - accuracy: 0.3832 - loss: 3.6516\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 593ms/step - accuracy: 0.7922 - loss: 0.8292\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 585ms/step - accuracy: 0.8162 - loss: 0.7192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the base model's input and output layers\n",
        "feature_extraction_model2 = Model(inputs=base_model2.input, outputs= base_model2.output)"
      ],
      "metadata": {
        "id": "xGSWz-mdAnv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your data is preprocessed and batched\n",
        "features2 = feature_extraction_model2.predict(train_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5m90eVXBUUa",
        "outputId": "5eada007-c869-4857-852f-e040da45778b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 670ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features2.shape)\n",
        "print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhlma2emDsN_",
        "outputId": "aa696ca8-34ae-498f-c242-e051579c0a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(193, 5, 4, 2048)\n",
            "(981, 2, 2, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Flatten\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assume game_features and face_features are your extracted features\n",
        "game_features = features2  # Example shape\n",
        "face_features = features  # Example shape\n",
        "\n",
        "# Pad the facial features to match the game features sequence length\n",
        "face_features_padded = pad_sequences(face_features, maxlen=5, dtype='float32', padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "zSD0ltzqBcqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape the features to have the same sequence length\n",
        "game_features_reshaped = game_features.reshape(game_features.shape[0], game_features.shape[1], -1)  # (193, 5, 4*2048)\n",
        "face_features_reshaped = face_features_padded.reshape(face_features_padded.shape[0], face_features_padded.shape[1], -1)  # (981, 5, 2*2048)\n",
        "\n",
        "# Align the number of samples\n",
        "min_samples = min(game_features_reshaped.shape[0], face_features_reshaped.shape[0])\n",
        "\n",
        "# Truncate both feature sets to the minimum sample size\n",
        "game_features_aligned = game_features_reshaped[:min_samples]\n",
        "face_features_aligned = face_features_reshaped[:min_samples]"
      ],
      "metadata": {
        "id": "81nrefDTDiju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine learnt features\n",
        "combined_features = np.concatenate((game_features_aligned, face_features_aligned), axis=-1)\n"
      ],
      "metadata": {
        "id": "ck6WTJ2kE-73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = combined_features.shape[0]  # Number of samples after truncation\n",
        "num_classes = 7\n",
        "labels = np.zeros(num_samples)\n",
        "labels[num_samples // 2:] = 1\n"
      ],
      "metadata": {
        "id": "WKso7-7ZPHb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "model4.add(LSTM(256, return_sequences=True, input_shape=(combined_features.shape[1], combined_features.shape[2])))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(LSTM(128))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(1, activation='sigmoid'))  # Adjust output layer based on task for future adjust according to future dataset classes\n",
        "\n",
        "# Compile the model\n",
        "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4.fit(combined_features, labels, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDz17UP4OmTF",
        "outputId": "55b7f3d3-357e-49fc-86db-4e4e85cddcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 746ms/step - accuracy: 0.5322 - loss: 0.7587\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 585ms/step - accuracy: 0.6533 - loss: 0.6341\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 709ms/step - accuracy: 0.6558 - loss: 0.5980\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 659ms/step - accuracy: 0.7630 - loss: 0.5264\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 723ms/step - accuracy: 0.8066 - loss: 0.4721\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d36646e3e20>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "xbQOjw2FarfW",
        "outputId": "702c02cb-0855-4ab0-b9ec-0419d10875b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_26 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │      \u001b[38;5;34m12,846,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_27 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m197,120\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,846,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m39,129,989\u001b[0m (149.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,129,989</span> (149.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,043,329\u001b[0m (49.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,043,329</span> (49.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m26,086,660\u001b[0m (99.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,086,660</span> (99.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUDhB6RLa0uv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}