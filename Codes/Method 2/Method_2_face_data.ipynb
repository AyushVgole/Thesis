{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D"
      ],
      "metadata": {
        "id": "cSQaI3AW6ygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset1_path = '/content/drive/MyDrive/Untitled folder/Custom dataset/Face/Preporcessed'\n",
        "dataset3_path = '/content/drive/MyDrive/Untitled folder/CK+48/CK+48'\n",
        "\n",
        "# Define the image dimensions\n",
        "img_width, img_height = 48,48\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerator instances for both datasets\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "train_generator1 = datagen.flow_from_directory(\n",
        "    dataset1_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_generator3 = datagen.flow_from_directory(\n",
        "    dataset3_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVz-X7bT62cp",
        "outputId": "ae658fb6-09be-4479-d94a-b8c546b68ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 images belonging to 7 classes.\n",
            "Found 981 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create combined generator\n",
        "def combined_generator(gen1, gen2):\n",
        "    while True:\n",
        "        try:\n",
        "            x1, y1 = next(gen1)  # Try to get data from generator1\n",
        "        except StopIteration:\n",
        "            x1, y1 = None, None  # If generator1 is exhausted, set to None\n",
        "        try:\n",
        "            x2, y2 = next(gen2)  # Try to get data from generator2\n",
        "        except StopIteration:\n",
        "            break  # If generator2 is exhausted, break the loop\n",
        "\n",
        "        if x1 is not None and x2 is not None:  # If both generators have data\n",
        "            yield np.concatenate([x1, x2]), np.concatenate([y1, y2])\n",
        "\n",
        "combined_generator = combined_generator(train_generator1, train_generator3)\n",
        "\n",
        "# import required libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "\n"
      ],
      "metadata": {
        "id": "jes7t5Ux6ndP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
      ],
      "metadata": {
        "id": "1sTtgMRw7bpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGG16 model, excluding the top layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom top layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(7, activation='softmax')(x)  # Adjust the number of classes as needed\n",
        "\n",
        "# Create the final model\n",
        "model6 = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "for _ in range(len(train_generator1) + len(train_generator3)):\n",
        "    next(combined_generator)  # Consume data until exhaustion\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-KeOtKs0kBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model6.fit(combined_generator,\n",
        "          steps_per_epoch=max(len(train_generator1), len(train_generator3)),\n",
        "          epochs=10,\n",
        "          validation_data=combined_generator,\n",
        "          validation_steps=max(len(train_generator1), len(train_generator3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpz4QAjS3C27",
        "outputId": "2d481382-f9ac-47df-a195-730fee354a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 4s/step - accuracy: 0.2756 - loss: 1.9418 - val_accuracy: 0.5559 - val_loss: 1.3597\n",
            "Epoch 2/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 4s/step - accuracy: 0.5441 - loss: 1.3543 - val_accuracy: 0.6659 - val_loss: 1.1123\n",
            "Epoch 3/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.6325 - loss: 1.1427 - val_accuracy: 0.6916 - val_loss: 0.9593\n",
            "Epoch 4/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.6720 - loss: 1.0046 - val_accuracy: 0.7412 - val_loss: 0.8655\n",
            "Epoch 5/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.6996 - loss: 0.9254 - val_accuracy: 0.7434 - val_loss: 0.7902\n",
            "Epoch 6/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 4s/step - accuracy: 0.7211 - loss: 0.8284 - val_accuracy: 0.7803 - val_loss: 0.7282\n",
            "Epoch 7/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.7329 - loss: 0.7911 - val_accuracy: 0.8093 - val_loss: 0.6525\n",
            "Epoch 8/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.7510 - loss: 0.7319 - val_accuracy: 0.8310 - val_loss: 0.6019\n",
            "Epoch 9/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.7551 - loss: 0.7234 - val_accuracy: 0.8383 - val_loss: 0.5734\n",
            "Epoch 10/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.7705 - loss: 0.6839 - val_accuracy: 0.8500 - val_loss: 0.5337\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ff563848fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "model6.save('Model6custom_CK.h5')\n",
        "# Save the model to a file\n",
        "model6.save('Model6Custom_CK.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TPI4N21WXWc",
        "outputId": "296c9d56-2ae4-4c3d-8b16-6b7c05c37788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD ANOTHER datasets\n",
        "dataset1_path = '/content/drive/MyDrive/Untitled folder/Custom dataset/Face/Preporcessed'\n",
        "dataset3_path = '/content/drive/MyDrive/Untitled folder/FER2013/FER2013/train'\n",
        "\n",
        "# Define the image dimensions\n",
        "img_width, img_height = 48,48\n",
        "\n",
        "# Define the batch size\n",
        "batch_size =256\n",
        "\n",
        "# Create ImageDataGenerator instances for both datasets\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "train_generator1 = datagen.flow_from_directory(\n",
        "    dataset1_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_generator4 = datagen.flow_from_directory(\n",
        "    dataset3_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tTQsIDQ7pmk",
        "outputId": "218ff12a-1409-4887-e07b-67ad90733317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 images belonging to 7 classes.\n",
            "Found 28709 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creat combined generator\n",
        "def combined_generator(gen1, gen2):\n",
        "    while True:\n",
        "        try:\n",
        "            x1, y1 = next(gen1)  # Try to get data from generator1\n",
        "        except StopIteration:\n",
        "            x1, y1 = None, None  # If generator1 is exhausted, set to None\n",
        "        try:\n",
        "            x2, y2 = next(gen2)  # Try to get data from generator2\n",
        "        except StopIteration:\n",
        "            break  # If generator2 is exhausted, break the loop\n",
        "\n",
        "        if x1 is not None and x2 is not None:  # If both generators have data\n",
        "            yield np.concatenate([x1, x2]), np.concatenate([y1, y2])\n",
        "\n",
        "combined_generator = combined_generator(train_generator1, train_generator4)\n",
        "\n",
        "# import required libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "\n"
      ],
      "metadata": {
        "id": "HIrqP0NF8QXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
      ],
      "metadata": {
        "id": "hr0V3bHK8W3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGG16 model, excluding the top layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom top layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(7, activation='softmax')(x)  # Adjust the number of classes as needed\n",
        "\n",
        "# Create the final model\n",
        "model7 = Model(inputs=base_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "HgOuppyU8a_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model7.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "for _ in range(len(train_generator1) + len(train_generator4)):\n",
        "    next(combined_generator)"
      ],
      "metadata": {
        "id": "-YzYwpp-NkHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model7.fit(combined_generator,\n",
        "          steps_per_epoch=max(len(train_generator1), len(train_generator3)),\n",
        "          epochs=5,\n",
        "          validation_data=combined_generator,\n",
        "          validation_steps=max(len(train_generator1), len(train_generator3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25544DYG8i19",
        "outputId": "bc9e1854-52ad-4dea-be58-ef84b5f7a205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 21s/step - accuracy: 0.2048 - loss: 1.9141 - val_accuracy: 0.2317 - val_loss: 1.8280\n",
            "Epoch 2/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 21s/step - accuracy: 0.2310 - loss: 1.8476 - val_accuracy: 0.2286 - val_loss: 1.8109\n",
            "Epoch 3/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 21s/step - accuracy: 0.2291 - loss: 1.8428 - val_accuracy: 0.2307 - val_loss: 1.8091\n",
            "Epoch 4/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 20s/step - accuracy: 0.2414 - loss: 1.8273 - val_accuracy: 0.2345 - val_loss: 1.7990\n",
            "Epoch 5/5\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 21s/step - accuracy: 0.2395 - loss: 1.8267 - val_accuracy: 0.2321 - val_loss: 1.8084\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ff564de8220>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "model7.save('Model7custom_fer.h5')\n",
        "# Save the model to a file\n",
        "model7.save('Model7Custom_fer.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkW4sfFOfSmH",
        "outputId": "07f1d755-060c-4cb9-b2f8-3b0883c9ab32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHXryfUS1sqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}